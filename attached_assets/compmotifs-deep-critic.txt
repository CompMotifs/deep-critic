Directory structure:
└── compmotifs-deep-critic/
    ├── README.md
    ├── requirements.txt
    ├── .env_template
    ├── api/
    │   ├── main.py
    │   ├── converters/
    │   │   └── pdf_to_markdown.py
    │   ├── llms/
    │   │   ├── claude_client.py
    │   │   ├── mistral_client.py
    │   │   └── openai_client.py
    │   ├── prompts/
    │   │   ├── Prompt
    │   │   └── review_prompt.py
    │   └── routes/
    │       └── reviews.py
    └── parse_output/
        ├── README.md
        ├── pyproject.toml
        ├── outputparser/
        │   ├── __init__.py
        │   ├── converter.py
        │   ├── review_processor.py
        │   └── structure.py
        └── tests/
            ├── __init__.py
            ├── test_converter.py
            └── test_review_processor.py

================================================
File: README.md
================================================
# Deep Critic Backend API

This document describes how to perform a quick test of the Deep Critic backend API.

## Setup

Install the required dependencies by running:

```bash
pip install -r requirements.txt
```

Ensure you have configured your .env file properly! See .env_template in the repo, copy it to .env, and fill it out. 

## Starting the Server

Start the FastAPI backend server using:

```bash
uvicorn api.main:app --reload
```

## Quick API Test

To verify the backend API is running correctly, send a POST request using the following `curl` command:

```bash
curl -X POST -H "Content-Type: application/json" \
-d '{"paper_text": "Sample paper text or abstract here..."}' \
http://localhost:8000/api/review
```

### Expected Response

A successful response returns structured JSON data containing reviews from the integrated Large Language Models (LLMs):

```json
{
  "openai": { "summary": "...", "strengths": [...], "weaknesses": [...], "scores": {...} },
  "claude": { "summary": "...", "strengths": [...], "weaknesses": [...], "scores": {...} },
  "mistral": { "summary": "...", "strengths": [...], "weaknesses": [...], "scores": {...} }
}
```

## Test pdf convert and LLM review
Replace the file path with a path to a small pdf locally. 

```bash
curl -X POST \
-F "pdf_file=@/Users/jgerold/coding/compute-for-science/short_paper.pdf" \
http://localhost:8000/api/upload-and-review
```

### Troubleshooting

If the response contains errors, ensure that:
- API keys are correctly defined in your `.env` file.
- The FastAPI backend server is running (`uvicorn api.main:app --reload`).
- The Python environment includes all required dependencies (`pip install -r requirements.txt`).




================================================
File: requirements.txt
================================================
fastapi
uvicorn[standard]
openai==1.55.3
anthropic
requests
mistralai
python-dotenv
marker-pdf
python-multipart


================================================
File: .env_template
================================================
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-claude-key
MISTRAL_API_KEY=your-huggingface-token


================================================
File: api/main.py
================================================
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api.routes.reviews import router as review_router

app = FastAPI()

# Allow cross-origin requests (adjust in production)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register API route
app.include_router(review_router, prefix="/api")



================================================
File: api/converters/pdf_to_markdown.py
================================================
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

def convert_pdf_to_markdown(pdf_path):
    converter = PdfConverter(
    artifact_dict=create_model_dict(),
    )

    # Run converter on PDF
    rendered = converter(pdf_path)

    # Get text (markdown), images, tables
    markdown_text, _, images = text_from_rendered(rendered)

    return markdown_text, images




================================================
File: api/llms/claude_client.py
================================================
import anthropic
import os
from dotenv import load_dotenv

load_dotenv()

anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

def get_claude_review(paper_text, prompt):
    message = anthropic_client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1000,
        temperature=0.3,
        messages=[
            {"role": "user", "content": f"{prompt}\n\nPaper:\n{paper_text}"}
        ],
    )
    return message.content[0].text.strip()



================================================
File: api/llms/mistral_client.py
================================================
import os
from mistralai import Mistral
from dotenv import load_dotenv

load_dotenv()

MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
client = Mistral(api_key=MISTRAL_API_KEY)

def get_mistral_review(paper_text, prompt):
    messages = {
        'role': 'user',
        'content': f"{prompt}\n\nPaper:\n{paper_text}"
    }
    chat_response = client.chat.complete(
        model="mistral-small-latest",
        messages=messages
    )

    return chat_response.choices[0].message.content.strip()



================================================
File: api/llms/openai_client.py
================================================
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

def get_openai_review(paper_text, prompt):
    messages = [
        {"role": "user", "content": f"{prompt}\n\nPaper:\n{paper_text}"}
    ]
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,
    )
    return response.choices[0].message.content.strip()



================================================
File: api/prompts/Prompt
================================================
class Prompt:
      #!/usr/bin/env python
      # coding: utf-8

      # # Prompts and guidelines for writing a NeurIPS review
      # 
      # What a NeurIPS prompt looks like: https://openreview.net/forum?id=aVh9KRZdRk
      # Original prompts come from: https://neurips.cc/Conferences/2020/PaperInformation/ReviewerGuidelines

      # In[ ]:


      # Reviewer best practices
      # Prompt: The following text acts as a guideline for best practices when reviewing papers for the 2025 NeurIPS conference. Please remember these
      # guidelines when performing the reviews. Do not return an answer to this prompt.

      reviewer_best_practices = ('Reviewer best practices \n It is okay to be unavailable for part of the review process (e.g., on vacation for a few days), but if you will be unavailable for more than that -- especially during important windows (e.g., discussion) -- you must let your ACs know. \n With great power comes great responsibility! Take your job seriously and be fair. Write thoughtful and constructive reviews. Although the double-blind review process reduces the risk of discrimination, reviews can inadvertently contain subtle discrimination, which should be actively avoided. \n Example: avoid comments regarding English style or grammar that may be interpreted as implying the author is "foreign" or "non-native". So, instead of "Please have your submission proof-read by a native English speaker,” use a neutral formulation such as "Please have your submission proof-read for English style and grammar issues.” \n DO NOT talk to other reviewers, ACs, or SACs about submissions that are assigned to you without prior approval from your AC; other reviewers, ACs, and SACs may have conflicts with these submissions. In general, your primary point of contact for any discussions should be the corresponding AC for that submission. \n DO NOT talk to other reviewers, ACs, or SACs about your own submissions (i.e., submissions you are an author on) or submissions with which you have a conflict. \n Keep papers assigned to you in absolute confidentiality. \n Be professional, polite, and listen to the other reviewers, but do not give in to undue influence. \n Engage actively in the discussion phase for each of the submissions that you are assigned, even if you are not specifically prompted to do so by the AC. \n It is not fair to dismiss any submission without having thoroughly read it. Think about the times when you received an unfair, unjustified, short, or dismissive review. Try not to be that reviewer! Always be constructive and help the authors understand your viewpoint, without being dismissive or using inappropriate language. If you need to cite existing work to justify one of your comments, please be as precise as possible and give a complete citation. \n If you would like the authors to clarify something during the author response phase, please articulate this clearly in your review (e.g., “I would like to see results of experiment X” or “Can you please include details about the parameter settings used for experiment Y”).' )


      # In[3]:


      # Reviewer Instructions
      # Prompt: Continue remembering the following instructions. Do not return an answer to this prompt.

      reviewer_doubleblind = ('Double-blind reviewing \n Paper should not include author names, author affiliations, or acknowledgements in their submissions and they should avoid providing any other identifying information. Under no circumstances should you attempt to find out the identities of the authors for any of your assigned submissions (e.g., by searching on Google or arXiv). If you accidentally find out, please do not divulge the identities to anyone. You should not let the authors’ identities influence your decision in any way. If you see names ignore them.')

      reviewer_supplementary_material = ('Supplementary material \n Your responsibility as a reviewer is to read and review the submission itself; looking at supplementary material is at your discretion. That said, submissions are short, so you may wish to look at supplementary material before criticizing a submission for insufficient details, proofs, or experimental results.')

      reviewer_formatting = ('Formatting instructions \n Submissions are limited to eight content pages, including all figures and tables, in the “submission” style; additional pages containing only a) discussion of broader impact and b) references are allowed. If you are assigned any submissions that violate the NeurIPS style (e.g., by decreasing margins or font size) or page limits. If you detect ths highlight it in your feedback as a strong limit')

      reviewer_dual_submissions = ('Dual submissions \n Submissions that are identical or substantially similar to other submissions should also be deemed dual submissions.')


      # also concatenate all of the above to get one long_reviewer_instructions


      # In[4]:


      # Rules for reviewing content
      content_review_rules = ('Review content \n Please make your review as detailed and informative as possible; short, superficial reviews that venture uninformed opinions or guesses are worse than no review since they may result in the rejection of a high-quality submission.')

      # Prompt (stored in content_review_rules to an extent, but also we may need a separate prompt)
      # Prompt: From now on, please consider all of the following and return an answer to each prompt.
      q1_summary_statement = ('1. Summary and contributions: Summarize the paper motivation, key contributions and achievements in a paragraph.')

      q2_strengths_statement = ('2. Strengths: List the strengths of the submission. For instance, it could be about the soundness of the theoretical claim or the soundness of empirical methodology used to validate an empirical approach. Another important axis is the significance and the novelty of the contributions relative to what has been done already in the literature, and here you may want to cite these relevant prior works. One measure of the significance of a contribution is (your belief about) the level to which researchers or practitioners will make use of or be influenced by the proposed ideas. Solid, technical papers that explore new territory or point out new directions for research are preferable to papers that advance the state of the art, but only incrementally. Finally, a possible strength is the relevance of the line of work for the community.')

      q3_weaknesses_statement = ('3. Weaknesses: This is like above, but now focussing on the limitations of this work. Your comments should be detailed, specific, and polite. Please avoid vague, subjective complaints. Think about the times when you received an unfair, unjustified, short, or dismissive review. Try not to be that reviewer! Always be constructive and help the authors understand your viewpoint, without being dismissive or using inappropriate language. Remember that you are not reviewing your level of interest in the submission, but its scientific contribution to the field!')

      q4_correctness_statement = ('4. Correctness: Are the claims and method correct? Is the empirical methodology correct? \n Explain if there is anything incorrect with the paper. Incorrect claims or methodology are the primary reason for rejection. Be as detailed, specific and polite as possible. Thoroughly motivate your criticism so that authors will understand your point of view and potentially respond to you.')

      q5_clarity_statement = ('5. Clarity: Is the paper well written? \n Rate the clarity of exposition of the paper. Give examples of what parts of the paper need revision to improve clarity.')

      q6_relation_to_prior_work_statement = ('6. Relation to prior work: Is it clearly discussed how this work differs from previous contributions? \n Explain whether the submission is written with the due scholarship, relating the proposed work with the prior work in the literature. The related work section should not just list prior work, but explain how the proposed work differs from prior work appeared in the literature. \n Note that authors are excused for not knowing about all non-refereed work (e.g, those appearing on ArXiv). Papers (whether refereed or not) appearing less than two months before the submission deadline are considered contemporaneous to NeurIPS submissions; authors are not obligated to make detailed comparisons to such papers (though, especially for the camera ready versions of accepted papers, authors are encouraged to).')

      q7_reproducibility_statement = ('7. Reproducibility: Are there enough details to reproduce the major results of this work? \n Mark whether the work is reasonably reproducible. If it is not, lack of reproducibility should be listed among the weaknesses of the submission.')

      q8_additional_feedback_statement = ('8. Additional feedback, comments, suggestions for improvement and questions for the authors \n Add here any additional comment you might have about the submission, including questions and suggestions for improvement.')

      q9_overall_score_statement = ('9. Overall score: \n You should NOT assume that you were assigned a representative sample of submissions, nor should you adjust your scores to match the overall conference acceptance rates. The “Overall Score” for each submission should reflect your assessment of the submissions contributions. \n 10: Top 5% of accepted NeurIPS papers. Truly groundbreaking work. \n 9: Top 15% of accepted NeurIPS papers. An excellent submission; a strong accept. \n 8: Top 50% of accepted NeurIPS papers. A very good submission; a clear accept. \n 7: A good submission; accept - I vote for accepting this submission, although I would not be upset if it were rejected. \n 6: Marginally above the acceptance threshold - I tend to vote for accepting this submission, but rejecting it would not be that bad. \n 5: Marginally below the acceptance threshold - I tend to vote for rejecting this submission, but accepting it would not be that bad. \n 4: An okay submission, but not good enough; a reject - I vote for rejecting this submission, although I would not be upset if it were accepted. \n 3: A clear reject - I vote and argue for rejecting this submission. \n 2: Im surprised this work was submitted to NeurIPS; a strong reject - \1: Trivial or wrong or already known. ')

      q10_confidence_score_statement = ('10.  Confidence score: \n 5: You are absolutely certain about your assessment. You are very familiar with the related work. \n 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. \n 3: You are fairly confident in your assessment.  It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked. \n 2: You are willing to defend your assessment, but it is quite likely that you did not understand central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked. \n 1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.')

      q11_broader_impact_statement = ('11. Broader impact: Have the authors adequately addressed the broader impact of their work, including potential negative ethical and societal implications of their work? \n Yes, no or only partially. In order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes. Indicate whether you believe the broader impact section was adequate.')

      q12_ethical_concerns_statement = ('12. Does the submission raise potential ethical concerns? This includes methods, applications, or data that create or reinforce unfair bias or that have a primary purpose of harm or injury. If so, please explain briefly. \n Yes or No. Explain if the submission might raise any potential ethical concern. Note that your rating should be independent of this. If the AC also shares this concern, dedicated reviewers with expertise at the intersection of ethics and ML will further review the submission. Your duty here is to flag only papers that might need this additional revision step.')

      # concatenate all statements to get one big statement to respond to all queries






================================================
File: api/prompts/review_prompt.py
================================================
import Prompt
REVIEW_PROMPT = Prompt.Prompt()



================================================
File: api/routes/reviews.py
================================================
from concurrent.futures import ThreadPoolExecutor
from fastapi import APIRouter, UploadFile, File, HTTPException
import os
from api.converters.pdf_to_markdown import convert_pdf_to_markdown
from api.llms.openai_client import get_openai_review
from api.llms.claude_client import get_claude_review
from api.llms.mistral_client import get_mistral_review
from api.prompts.review_prompt import REVIEW_PROMPT
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered
import io
import json
import re
import tempfile

router = APIRouter()

def parse_json_response(response_text):
    # Strip markdown code block if present
    json_match = re.search(r"```json\n(.*?)\n```", response_text, re.DOTALL)
    if json_match:
        response_text = json_match.group(1)

    return json.loads(response_text)

def convert_pdf_bytes_to_markdown(pdf_bytes):
    with tempfile.NamedTemporaryFile(suffix=".pdf") as tmp_pdf:
        tmp_pdf.write(pdf_bytes)
        tmp_pdf.flush()

        converter = PdfConverter(artifact_dict=create_model_dict())

        # Run converter on the temporary PDF file path
        rendered = converter(tmp_pdf.name)

        # Get text (markdown), images, tables
        markdown_text, _, images = text_from_rendered(rendered)

    return markdown_text, images

@router.post("/upload-and-review")
async def upload_and_review(pdf_file: UploadFile = File(...)):
    # Read PDF content directly into memory
    content = await pdf_file.read()

    # Convert PDF to Markdown directly from memory
    try:
        markdown_text, images = convert_pdf_bytes_to_markdown(content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"PDF conversion failed: {str(e)}")

    # Call LLM APIs in parallel for fast response
    with ThreadPoolExecutor() as executor:
        futures = {
            "openai": executor.submit(get_openai_review, markdown_text, REVIEW_PROMPT),
            "claude": executor.submit(get_claude_review, markdown_text, REVIEW_PROMPT),
            "mistral": executor.submit(get_mistral_review, markdown_text, REVIEW_PROMPT),
        }

    # Gather and parse responses
    results = {}
    for name, future in futures.items():
        try:
            response_text = future.result()
            results[name] = parse_json_response(response_text)
        except json.JSONDecodeError:
            results[name] = {"error": "Invalid JSON response", "raw": response_text}
        except Exception as e:
            results[name] = {"error": str(e)}

    return results



================================================
File: parse_output/README.md
================================================
# Output Parser
Implements the LLM feedback parser and converter for generating OpenReview-style reviews from multiple LLM agents. The tool parses raw feedback (with sections such as Summary, Soundness, Presentation, Contribution, Strengths, Weaknesses, Questions, Limitations, Rating), aggregates scores from 10 LLMs, computes a consensus-based Confidence score, and then uses another LLM (via an API call) to generate the final review.

## Structure

- **src/parser.py**: Contains functions to parse individual LLM outputs.
- **src/converter.py**: Implements the conversion step using an LLM API (or simulated function).
- **src/review_processor.py**: Orchestrates parsing, aggregation, and conversion.
- **tests/**: Unit tests for each module.

## Requirements

See [requirements.txt](requirements.txt).

## Setup

Install dependencies with:
```bash
pip install -r requirements.txt
```

Install the package:
```bash
pip install -e .
```

## Run tests
```bash
python -m unittest discover tests
```



================================================
File: parse_output/pyproject.toml
================================================
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "outputparser"
version = "0.1.0"
description = "LLM feedback parser and converter for generating OpenReview-style reviews from multiple LLM agents."
readme = "README.md"
dependencies = [
  "openai>=0.27.0",
  "numpy",
  "regex"
]
classifiers = [
  "Programming Language :: Python :: 3",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent"
]



================================================
File: parse_output/outputparser/__init__.py
================================================



================================================
File: parse_output/outputparser/converter.py
================================================
from openai import OpenAI
from outputparser.structure import Review


def call_conversion_llm(context: str, prompt: str) -> str:
    """
    Call an LLM to convert the aggregated feedback into an OpenReview style review.
    """
    client = OpenAI()

    completion = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": context},
            {"role": "user", "content": prompt},
        ],
        response_format=Review,
    )
    return completion.choices[0].message.parsed


def convert_to_openreview(aggregated_data: dict) -> str:
    """
    Convert aggregated feedback data into an OpenReview style review.
    aggregated_data should have keys:
      summary, soundness, presentation, contribution,
      strengths, weaknesses, questions, limitations, rating, confidence
    """
    # Create a prompt that includes all the aggregated details.
    context = (
        "Using the following aggregated feedback, produce a final OpenReview style review "
        "with the sections: Summary; Soundness, Presentation and Contribution (scores from 1 to 5); "
        "Strengths and Weaknesses; Questions; Limitations; Rating (score from 1 to 10); Confidence."
    )
    prompt = (
        f"Summary: {aggregated_data.get('summary')}\n"
        f"Soundness: {aggregated_data.get('soundness')}\n"
        f"Presentation: {aggregated_data.get('presentation')}\n"
        f"Contribution: {aggregated_data.get('contribution')}\n"
        f"Strengths: {aggregated_data.get('strengths')}\n"
        f"Weaknesses: {aggregated_data.get('weaknesses')}\n"
        f"Questions: {aggregated_data.get('questions')}\n"
        f"Limitations: {aggregated_data.get('limitations')}\n"
        f"Rating: {aggregated_data.get('rating')}\n"
        f"Confidence: {aggregated_data.get('confidence')}\n"
    )
    return call_conversion_llm(context, prompt)



================================================
File: parse_output/outputparser/review_processor.py
================================================
import numpy as np
from outputparser.converter import convert_to_openreview
from outputparser.structure import Review


def aggregate_feedback(feedbacks: Review) -> dict:
    """
    Aggregate parsed feedback from multiple LLM responses.
    For numeric scores, computes the average.
    For text sections, combines the text.
    Computes a confidence score based on the standard deviation of numeric scores.
    """
    # Initialize lists for numeric scores
    soundness_scores = []
    presentation_scores = []
    contribution_scores = []
    rating_scores = []

    summaries = []
    strengths_list = []
    weaknesses_list = []
    questions_list = []
    limitations_list = []

    for i, fb in enumerate(feedbacks):
        soundness_scores.append(fb.soundness)
        presentation_scores.append(fb.presentation)
        contribution_scores.append(fb.contribution)
        rating_scores.append(fb.rating)

        summaries.append(f"LLM {i+1}: {fb.summary}")
        strengths_list.append(f"LLM {i+1}: {fb.strengths}")
        weaknesses_list.append(f"LLM {i+1}: {fb.weaknesses}")
        questions_list.append(f"LLM {i+1}: {fb.questions}")
        limitations_list.append(f"LLM {i+1}: {fb.limitations}")

    # Compute averages
    aggregated = {}
    aggregated["soundness"] = (
        round(np.mean(soundness_scores)) if soundness_scores else None
    )
    aggregated["presentation"] = (
        round(np.mean(presentation_scores)) if presentation_scores else None
    )
    aggregated["contribution"] = (
        round(np.mean(contribution_scores)) if contribution_scores else None
    )
    aggregated["rating"] = round(np.mean(rating_scores)) if rating_scores else None

    # Combine text sections
    aggregated["summary"] = " ".join(summaries)
    aggregated["strengths"] = " ".join(strengths_list)
    aggregated["weaknesses"] = " ".join(weaknesses_list)
    aggregated["questions"] = " ".join(questions_list)
    aggregated["limitations"] = " ".join(limitations_list)

    # Compute confidence based on agreement of numeric scores.
    # Here we compute the mean standard deviation of the three scores.
    stds = []
    for scores in (soundness_scores, presentation_scores, contribution_scores):
        if scores:
            stds.append(np.std(scores))
    if stds:
        avg_std = np.mean(stds)
        # A lower std means higher confidence. Here we map average std to a confidence score out of 10.
        confidence = max(0, round((1 / (1 + avg_std)) * 10, 1))
    else:
        confidence = None
    aggregated["confidence"] = confidence

    return aggregated


def process_llm_feedbacks(feedbacks: list) -> str:
    """
    Process raw feedback texts from multiple LLMs and produce the final OpenReview style review.
    """
    aggregated_data = aggregate_feedback(feedbacks)
    final_review = convert_to_openreview(aggregated_data)
    return final_review



================================================
File: parse_output/outputparser/structure.py
================================================
from pydantic import BaseModel


class Review(BaseModel):
    summary: str
    soundness: int
    presentation: int
    contribution: int
    strengths: str
    weaknesses: str
    questions: str
    limitations: str
    rating: int
    confidence: int



================================================
File: parse_output/tests/__init__.py
================================================



================================================
File: parse_output/tests/test_converter.py
================================================
import unittest
from outputparser.converter import convert_to_openreview
from outputparser.structure import Review


class TestConverter(unittest.TestCase):
    def test_convert_to_openreview(self):
        aggregated_data = {
            "summary": "Combined summary from multiple LLMs.",
            "soundness": 4,
            "presentation": 3,
            "contribution": 5,
            "strengths": "Good methodology.",
            "weaknesses": "Limited experiments.",
            "questions": "How does it scale?",
            "limitations": "Small datasets.",
            "rating": 8,
            "confidence": 9.0,
        }
        result = convert_to_openreview(aggregated_data)
        self.assertIsInstance(result, Review)


if __name__ == "__main__":
    unittest.main()



================================================
File: parse_output/tests/test_review_processor.py
================================================
import unittest
from outputparser.review_processor import process_llm_feedbacks
from outputparser.structure import Review


class TestReviewProcessor(unittest.TestCase):
    def test_process_llm_feedbacks(self):
        sample_feedback_1 = Review(
            summary="This work offers a novel perspective on XYZ.",
            soundness=4,
            presentation=3,
            contribution=5,
            strengths="Innovative idea.",
            weaknesses="Needs more experiments.",
            questions="What is the computational cost?",
            limitations="Not tested on large-scale data.",
            rating=8,
            confidence=8,
        )
        sample_feedback_2 = Review(
            summary="This work offers a perspective on XYZ.",
            soundness=2,
            presentation=2,
            contribution=1,
            strengths="Good idea.",
            weaknesses="Needs more experiments.",
            questions="What is the computational cost?",
            limitations="Not tested on large-scale data.",
            rating=7,
            confidence=6,
        )
        sample_feedback_3 = Review(
            summary="This work offers a novel perspective on XYZ.",
            soundness=3,
            presentation=4,
            contribution=3,
            strengths="Great idea.",
            weaknesses="Needs more experiments.",
            questions="What is the computational cost?",
            limitations="Not tested on large-scale data.",
            rating=9,
            confidence=7,
        )
        raw_feedbacks = [sample_feedback_1, sample_feedback_2, sample_feedback_3]
        final_review = process_llm_feedbacks(raw_feedbacks)
        self.assertIsInstance(final_review, Review)
        self.assertEqual(final_review.soundness, 3)
        self.assertEqual(final_review.presentation, 3)
        self.assertEqual(final_review.contribution, 3)
        self.assertEqual(final_review.rating, 8)


if __name__ == "__main__":
    unittest.main()


